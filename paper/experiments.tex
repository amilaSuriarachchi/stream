\section{Performance Benchmarks}

We conducted several performance tests to measure the throughput and scalability of our system. All these tests were performed on a LAN system called Lattice which has a network bandwidth of 1 Gbps. All nodes are Intel(R) Xeon(R) 2.4GHz 4 core duo machines with 16 GB of memory. We used Apache S4 0.6.0 version and Apache storm 0.9.2 version. Sample code used for all tests can be found here\cite{solutionCode}.


\subsection{Inter node communication}
We used a graph as shown in Figure \ref{ecgGraph} to process ECG signal data. Our solution \textit{EventProducer} and Yahoo S4\cite{neumeyer2010s4} \textit{EventProducer} read a file containing over 7500000 ECG records and push events with multiple threads. Twitter Storm\cite{twitterStorm} does not allow user threads to push data. So we used the \textit{spout} thread to send messages into the system. \textit{EventReceiver}  receives the ECG events, processes them and calculates heart rate interval periodically.  

\subsubsection{Throughput}
We executed our system with 1, 2 and 4  \textit{EventReceivers} for each system and measured the throughput, load average and network bandwidth for each case. Throughput was measured at the \textit{EventProducer} by calculating the total time required to send messages and the total number of messages send. Load average and network bandwidth were measured using top and atop linux commands respectively. Figure \ref{throuput} shows the throughput variation with the number of nodes for all systems. 

\begin{figure}[!t]
        \centering
        \includegraphics[width=3.0in]{ecgGraph.png}
        \caption{ECG Process Graph}
        \label{ecgGraph}
\end{figure}
\begin{figure}[!t]
        \centering
        \includegraphics[width=3.0in]{throughput.png}
        \caption{Throughput of the systems}
        \label{throuput}
\end{figure}

As shown in the Figure \ref{throuput}, our solution outperforms Twitter Storm \cite{twitterStorm} and Yahoo S4 \cite{neumeyer2010s4}. However by looking at the Figure \ref{throuput}, one might think even our solution is not scalable although it performs well. In order to examine the reason behind this, we looked into the network bandwidth and CPU load average values. 
 
Figure \ref{networkandload} shows the network bandwidth usage and the CPU load average at each node. For multiple node scenarios, since we observed the same values for both bandwidth usage and CPU load average at each receiver, we took the average values of them. 

\begin{figure*}[!t]
	\centering
	\subfloat[Network Usage]{\includegraphics[width=3.0in]{network.png}}
	\subfloat[CPU Load Average]{\includegraphics[width=3.0in]{loadaverage.png}}
	\caption{Network usage and CPU Load Average of the System}
	\label{networkandload}
\end{figure*}
 

By looking at the graphs, first it can be observed that our solution utilities all available network bandwidth (98\%) even with two receivers. In the one receiver case, high CPU load average at the receiver indicates that it has utilized available CPU power. Adding two receivers has increased the CPU power at receiving side utilizing all available network bandwidth. Therefore, we can not increase the throughput without increasing network bandwidth although more CPU capacity is available at the receiver nodes. Having more receiver nodes, we can observe that network bandwidth and CPU load average is reduced at each receiver, since load is shared among the receivers. However for other two systems, it neither hits the maximum network bandwidth available nor the full CPU power available at any stage. If a system does not make efficient use of the available resources then adding more resources will not scale up the system. As explained in earlier, our solution utilities all available resources by using parallel TCP connections and having efficient message transformations.

\subsubsection{Efficiency of Message Serialization}
After measuring the throughput, we examined the  total amount of extra bytes each system sends to transfer the information from \textit{EventProducer} to \textit{EventReceiver}. Our ECGEvent has two double fields called time and value (ecg signal value) and a streamID which is a 4 byte string. So we calculated total bytes to send this information as 20 bytes (16 for two doubles and 4 for streamID). Then by using the throughput and the bandwidth usage of the system, we calculated the total number of bytes each system uses at network layer to send this message. Finally we calculated the overhead bytes for each system by reducing 20 information bytes. Figure \ref{efficiency} compares these numbers.

\begin{figure}[!t]
        \centering
        \includegraphics[width=3.0in]{efficiency.png}
        \caption{Efficiency of Message Serialization}
        \label{efficiency}
\end{figure}

As shown in the figure \ref{efficiency}, Twitter Storm \cite{twitterStorm} uses a minimum over head while Yahoo S4 \cite{neumeyer2010s4} performs worse compared to other two. This overhead is due to its usage of Java serialization. We analyzed our solutions' extra overhead of 32 bytes. Our solution adds a sequence number (an integer), receiving process id (a 8 byte string for this scenario), sending process id (a 8 byte string for this scenario) to order messages, dispatch the message to correct process, and to parse the message at the server. Altogether it adds 26 (serialization format of a string uses two more bytes to keep the string length) extra bytes including 2 overhead bytes for streamID, to each message at application level and another 4 bytes due to TCP overhead. This is the trade off we had to pay to improve the parallelism compared to direct one TCP communication per process. 

\subsubsection{Latency}
We examined the message latency between the event producer and receiver. For this experiment, we sent the timestamp at the producer along the message and calculated the latency at the receiver. We took 6000 samples over the experiment duration. As shown in the figure \ref{latancydis}, we could observe the mean latency of 185.05ms with the 95\% confidence interval of (182.56ms, 187.54ms). 

\begin{figure}[!t]
        \centering
        \includegraphics[width=3.0in]{latencyHistogram.png}
        \caption{Message Latency Distribution}
        \label{latancydis}
\end{figure}


\subsection{Scalability}
We performed a scalability test for our system using the 3-level graph shown in Figure \ref{multigraph}. The data as well as the processing logic were obtained from the Grand Challenge problem at 8th ACM International Conference on Distributed Event Based Systems. We used the publicly available 500MB of data to generate the events. 

\begin{figure}[!t]
        \centering
        \includegraphics[width=3.0in]{multigraph.png}
        \caption{Multilevel Node Graph}
        \label{multigraph}
\end{figure}

This application predicts load averages using previous values and a machine learning algorithm. We implemented this logic using nodes as shown in the Figure \ref{multigraph}. The first producer reads the data file and sends events to the \textit{avgCal} node which calculates the last minute average and sends the same event to both \textit{plugPredict} and \textit{housePredict} processors. Both \textit{plugPredict} and \textit{housePredict} processors predict the next values and send events to receivers. The original problem only requires to send those prediction events in 30s intervals. But we used a prediction event for each messages to observe how the system works with high load.
 
As in the earlier case, we conducted our experiments using one producer and incrementing the other processing nodes by 1, 2 and 4 times to measure the throughput increase at the producer. We ran each node in a separate machine so that our receiver configurations used 5, 10, 15 machines respectively. We measured the throughput,  network bandwidth and the CPU load average at the producer to examine the scalability of the system. Since the one receiver unit throughput is greater than that of the Twitter Storm one node scenario, we only used our system for this experiment.  Results are shown in the Figure \ref{scalability}.


\begin{figure*}[!t]
        \centering
        \subfloat[Throughput]{\includegraphics[width=3.0in]{throughputs.png}}
        \hfil
        \subfloat[Network Usage]{\includegraphics[width=3.0in]{networkps.png}}
        \hfil
        \subfloat[Network Usage Bandwidth]{\includegraphics[width=3.0in]{networks.png}}
        \hfil
        \subfloat[CPU Load Average]{\includegraphics[width=3.0in]{loadaverages.png}}
        \caption{Scalability of the System}
        \label{scalability}
\end{figure*}


For one the receiver unit case, we observed very high load average and network bandwidth usage at \textit{avgCal} node since it sends messages to two nodes. As shown in  Figure \ref{scalability}, we could linearly scale up the system by adding more receiver units to add more cpu power to the system. When increasing the receiver units, we could observe throughput of the system increases proportional to number of receiving units. Our CPU load average of 13 at the producer indicates it has utilized all available CPU power and it is required to add more producers to scale this system further up. 


