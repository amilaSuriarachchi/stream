\section{Introduction}

Multi-stage, distributed stream processing is useful in analyzing data streams generated by programs, sensors, or users. Analysis of click-streams, tweets, and stock-quotes are primary examples where such processing is often performed. Most commonly, in such systems packets encapsulate \textit{tuples} representing values corresponding to a set of variables. One advantage of doing multi-stage processing is that individual stages can be scaled horizontally in response to the load i.e., each stage could have multiple instances.

The stages involved in such stream processing can be fluid.  It is possible for stages to be added and removed dynamically. Furthermore, different processing pipelines may share stages. Along the same lines, a data packet may be processed within multiple processing pipelines. Individual stages may transform the packets being processed before forwarding it to other stages. Stream processing systems do not place restrictions on the type of packet transformations and modifications that can be performed. 

The rates at which the stream packets arrive place unique strains on such systems. Each packet results in a mix of processing and I/O at each stage. Failure to keep up with the data generation rates result in queue build-ups, followed by overflows, and subsequent process failures. Furthermore, in the case of a multi-stage processing pipeline, the processing is only as fast as the slowest stage in the system.

There are two key aspects in processing stream packets: latency and throughput. Latency corresponds to the end-to-end delay as the packet makes its way through the processing pipeline. This metric is useful in characterizing the how timely the processing is. Throughput is a measure of how many packets can be processed per-second within a pipeline. The throughput represents how well the system can cope with rates of data arrivals. As the number of processing stages increase we expect to achieve higher throughput, though latency may increase a little corresponding to the number of hops within the pipeline.

\subsection{Research Challenges} 
In this paper we consider the problem of designing a scalable framework for the high throughput processing of data streams. There are several challenges in achieving this.
\begin{enumerate}
	\item Continuous data arrivals: In the systems we consider data is continually arriving at high rates. Inability to keep pace with the arrival rates will result in buffer overflows.
	\item Shared communication links: Stages comprising the stream processing pipeline may be distributed over multiple machines, and the links connecting these stages are shared Ethernet LANs. Effective utilization of these links is important for achieving high throughput.
	\item Commodity machines: Individual stages execute on commodity machines that have limited memory (order of a few GB) and processing cores (4).  So there are limits to the gains that can be accrued by processing a single packet faster.
\end{enumerate}

\subsection{Research Questions}
Support for high-throughput stream processing involves accounting for aspects relating to memory and processing at individual stages and also for communications between stages comprising the pipeline. Specific research questions that we explore include:
\begin{enumerate}
	\item How can we effectively manage memory consumption during processing? During packet processing, tuples must be extracted and processed. This involves allocation and garbage collection of memory. Given the rate at which packets arrive, operations relating memory management must be managed effectively.
	\item How can we ensure effective management of the processing workloads? Since each packet is processed independently, the framework must allow for multiple packets to be processed concurrently.
	\item How can we reduce latencies involved during processing? Since message passing between stages involves network I/O, we need to ensure that these I/O operations can be effectively interleaved with processing. Inefficiencies in interleaving result in serial processing that may cause queue build-ups.
\end{enumerate}

\subsection{Approach Summary}

The work described here provides a framework for multistage stream processing. Users are only required to specify the processing graph, connectivity between individual stages, and the processing to be performed by each stage. Our framework manages all aspects relating to memory management, efficiencies in communications, and concurrent processing.

Effective memory management is important to avoid buffer overflows and out-of-memory errors. We achieve this by reusing objects. Our approach precludes the need to create an object for every packet arrival. Given the rates at which data packets arrive, memory management costs relating to managing the object lifecycle (creation, initialization, and garbage collection) would be prohibitive. An additional artifact in creation of objects per packet is thrashing due to page faults as the memory consumption at a stage increases.  

Given that the stages are dispersed, message passing between stages comprising the pipeline must not be inefficient. Our effort targets effective utilization of the underlying network in two ways. First, we buffer application messages (messages produced by application logic). This buffering targets minimizing the creation of runtime objects for a large number messages while generating large enough messages to utilize the underlying network. The amount of messages get buffered can be configured to balance the competing pulls of timely delivery and high-throughput transfer. Second, our serialization scheme allows for compact over-the-wire representation of the tuples within the packets. Inefficiencies in serializations schemes result in verbose representations that increase the network footprint of individual packets.

Our framework also focuses on efficiencies in processing these packets. There are two ways in which we accomplish this. First, we rely on thread pools at each stage. Packet processing is performed within threads and the thread-pool is sized so that we balance concurrency gains and thread-switching overheads. Second, we harness non-blocking I/O. Given that packet processing at each stage involves network I/O the use of non-blocking I/O allows us to interleave processing and I/O much more effectively. 

We evaluate the suitability of our methodology by profiling its performance with real applications. This includes real-time ECG processing where we are analyzing waveform data in real-time to perform QRS-complex detection using the well-known Pan-Tompkins \cite{kohler2002principles} algorithm and a multi-stage query to analyze smart plug data. Finally, we also contrast the performance of our system with systems such as Twitter Storm \cite{toshniwal2014storm} and  Yahoo S4 \cite{neumeyer2010s4} that do not incorporate some of the efficiencies in our system. Our results validate the suitability of our approach with substantially higher throughputs that what can be achieved in these systems. Furthermore, several aspects of our methodology can be incorporated into these aforementioned systems to improve their performance as well. 


\subsection{Paper Contributions}
In this paper we have described how to achieve high-throughput, multi-stage distributed stream processing. Our specific contributions include the following:
\begin{enumerate}
	\item Our approach identifies aspects that play a key role in achieving high-throughput processing. High-throughput processing is possible by taking a holistic view of the system encompassing memory, computing, and communications efficiencies. This paper demonstrates how combining object reuse, serialization efficiencies, concurrent processing, buffering application messages, and the use of non-blocking I/O allows us to achieve high throughput. 
	\item The per-packet processing latencies that we achieve in our system demonstrate suitability for applications where such timely processing is necessary.
	\item We provide a simple framework for user to express their processing graph. Users only need to specify this graph and the processing that must be performed for each stage. Users are freed from the dealing with concurrent, I/O, and memory efficiency issues. 
	\item Our results demonstrate the suitability of our approach. We have contrasted the performance of our system with Twitter Storm \cite{toshniwal2014storm} and  Yahoo S4 \cite{neumeyer2010s4}.
\end{enumerate}

\subsection{Paper Organization}
The remainder of this paper is organized as follows: Section 2 provides an overview of our system at user level. Section 3 illustrates the underlying design of the system. Section 4 analyses the experiment results which demonstrate the performance of our solution. Section 5 discusses the related work and finally section 6 provides conclusions and future work.
